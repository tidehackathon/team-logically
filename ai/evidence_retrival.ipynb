{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0dbea60-8745-4285-b4db-76d200e2679a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E003] Not a valid pipeline component. Expected callable, but got 'sentencizer' (name: 'None').[E004] If you meant to add a built-in component, use `create_pipe`: `nlp.add_pipe(nlp.create_pipe('sentencizer'))`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# from nltk import sent_tokenize\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mnlp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_pipe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msentencizer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m nlp\u001b[38;5;241m.\u001b[39mdisable_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m nlp\u001b[38;5;241m.\u001b[39mdisable_pipe(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/home/david/venv/lib/python3.8/site-packages/spacy/language.py:337\u001b[0m, in \u001b[0;36mLanguage.add_pipe\u001b[0;34m(self, component, name, before, after, first, last)\u001b[0m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(component, basestring_) \u001b[38;5;129;01mand\u001b[39;00m component \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfactories:\n\u001b[1;32m    336\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m Errors\u001b[38;5;241m.\u001b[39mE004\u001b[38;5;241m.\u001b[39mformat(component\u001b[38;5;241m=\u001b[39mcomponent)\n\u001b[0;32m--> 337\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    339\u001b[0m     name \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mget_component_name(component)\n",
      "\u001b[0;31mValueError\u001b[0m: [E003] Not a valid pipeline component. Expected callable, but got 'sentencizer' (name: 'None').[E004] If you meant to add a built-in component, use `create_pipe`: `nlp.add_pipe(nlp.create_pipe('sentencizer'))`"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import pickle\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# from nltk import sent_tokenize\n",
    "nlp.add_pipe('sentencizer')\n",
    "nlp.disable_pipe(\"tagger\")\n",
    "nlp.disable_pipe(\"ner\")\n",
    "nlp.disable_pipe(\"attribute_ruler\")\n",
    "nlp.disable_pipe(\"lemmatizer\")\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def save_to_pkl(path, torch_embs):\n",
    "    with open(path, 'wb') as pkl:\n",
    "        pickle.dump(torch_embs, pkl)\n",
    "\n",
    "\n",
    "def save_to_npy(path, npy_embs):\n",
    "    np.save(path, npy_embs)\n",
    "\n",
    "\n",
    "def load_from_pkl(path):\n",
    "    with open(path, 'rb') as pkl:\n",
    "        embs = pickle.load(pkl)\n",
    "    return embs\n",
    "\n",
    "\n",
    "def load_from_npy(path):\n",
    "    return np.load(path)\n",
    "\n",
    "\n",
    "# sents = sent_tokenize(df_train.iloc[0]['document']) # nltk misses a lot of sentences if space does not occur after punctuation\n",
    "def sent_splitter(inp_df):\n",
    "    claim_id = 1\n",
    "    doc_id = 1\n",
    "    df_sents = pd.DataFrame()\n",
    "    for ind, row in inp_df.iterrows():\n",
    "        print(ind)\n",
    "        doc = nlp(row['document'])\n",
    "        for ind_sent, sentence in enumerate(doc.sents):\n",
    "            temp = row.copy()\n",
    "            temp['claim_id'] = claim_id\n",
    "            temp['doc_id'] = doc_id\n",
    "            temp['document'] = sentence.text\n",
    "            new_df = pd.DataFrame([temp])\n",
    "            df_sents = pd.concat([df_sents, new_df], axis=0, ignore_index=True)\n",
    "            doc_id += 1\n",
    "        claim_id += 1\n",
    "    df_sents.reset_index(drop=True, inplace=True)\n",
    "    # del df_val_sents_100['Unnamed: 0']\n",
    "    # len(df)\n",
    "    # print(df_sents.head())\n",
    "    return df_sents\n",
    "\n",
    "\n",
    "def generate_embeddings(df_sents, embedding_type='tensor', save_emb_name=\"val\"):\n",
    "    claims_l = df_sents['claim'].to_list()\n",
    "    print(len(claims_l))\n",
    "    docs_l = df_sents['document'].to_list()\n",
    "    print(len(docs_l))\n",
    "    if embedding_type == 'tensor':\n",
    "        claim_embeddings = model.encode(claims_l, convert_to_tensor=True)\n",
    "        doc_embeddings = model.encode(docs_l, convert_to_tensor=True)\n",
    "        save_to_pkl(save_emb_name + '_claim_text_sbert_embedding.pickle', claim_embeddings)\n",
    "        save_to_pkl(save_emb_name + '_doc_evid_text_sbert_embedding.pickle', doc_embeddings)\n",
    "\n",
    "    else:  # npy\n",
    "        claim_embeddings = model.encode(claims_l)\n",
    "        doc_embeddings = model.encode(docs_l)\n",
    "        save_to_npy(save_emb_name + '_claim_text_sbert_embedding.npy', claim_embeddings)\n",
    "        save_to_npy(save_emb_name + '_doc_evid_text_sbert_embedding.npy', doc_embeddings)\n",
    "\n",
    "    return claim_embeddings, doc_embeddings\n",
    "\n",
    "\n",
    "def load_embeddings(embedding_type='tensor', load_emb_name=\"val\"):\n",
    "    if embedding_type == 'tensor':\n",
    "        claim_embeddings = load_from_pkl(load_emb_name + '_claim_text_sbert_embedding.pickle')\n",
    "        doc_embeddings = load_from_pkl(load_emb_name + '_doc_evid_text_sbert_embedding.pickle')\n",
    "    else:\n",
    "        claim_embeddings = load_from_npy(load_emb_name + '_claim_text_sbert_embedding.npy')\n",
    "        doc_embeddings = load_from_npy(load_emb_name + '_doc_evid_text_sbert_embedding.npy')\n",
    "    return claim_embeddings, doc_embeddings\n",
    "\n",
    "\n",
    "def calc_sim(claim, doc_embs, encode_claim=False,\n",
    "             embedding_type='tensor'):  # Can pass a single claim embedding loaded from npy/pkl or single claim text\n",
    "    if encode_claim == True:\n",
    "        if embedding_type == 'tensor':\n",
    "            claim_emb = model.encode(claim, convert_to_tensor=True)\n",
    "        else:\n",
    "            claim_emb = model.encode(claim)\n",
    "    else:\n",
    "        claim_emb = claim\n",
    "\n",
    "    cosine_scores = util.cos_sim(claim_emb, doc_embs)\n",
    "    # sorted_res = sorted(range(len(cosine_scores[0].tolist())), key=lambda k: cosine_scores[k], reverse=True)[:TOP_K]\n",
    "    # sem_sim = util.semantic_search(claim_embeddings[45], doc_embeddings, top_k=TOP_K)\n",
    "    top_results = torch.topk(cosine_scores[0], k=TOP_K)\n",
    "    return top_results.indices.tolist()\n",
    "\n",
    "\n",
    "# GENERATE_SENTS = False\n",
    "# GENERATE_EMBS = False\n",
    "# SENTS_FILE_PATH = \"df_val_sents.csv\"\n",
    "# TOP_K = 25\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "\n",
    "#     if GENERATE_SENTS:\n",
    "#         # df_train = pd.read_csv(\"train.csv\", delimiter='\\t', encoding='utf-8')\n",
    "#         # print(len(df_train))\n",
    "#         df_val = pd.read_csv(\"val.csv\", delimiter='\\t', encoding='utf-8')\n",
    "#         print(len(df_val))\n",
    "#         df_sent = sent_splitter(df_val)\n",
    "#         df_sent.to_csv(SENTS_FILE_PATH)\n",
    "#     else:\n",
    "#         df_sent = pd.read_csv(SENTS_FILE_PATH)\n",
    "\n",
    "#     if GENERATE_EMBS:\n",
    "#         claim_embeddings, doc_embeddings = generate_embeddings(df_sent, embedding_type='npy', save_emb_name=\"val\")\n",
    "#     else:\n",
    "#         claim_embeddings, doc_embeddings = load_embeddings(embedding_type='tensor', load_emb_name=\"val\")\n",
    "\n",
    "#     sorted_ind = calc_sim(claim_embeddings[0], doc_embeddings,\n",
    "#                           encode_claim=False)  # Can pass a single claim embedding loaded from npy/pkl or single claim text\n",
    "#     ranked_evids_df = df_sent[sorted_ind]\n",
    "#     ranked_evids_df.to_csv(\"ranked_evids.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e4a4b52-5ca3-4999-92bc-6443373230fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(spacy.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
